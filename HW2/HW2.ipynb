{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89abd3b0-2a15-4b2f-aac8-dfa32422d215",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ТЗ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce397017-88d0-4146-a421-bdd40c04223f",
   "metadata": {
    "tags": []
   },
   "source": [
    "На последнем семинаре мы проанализировали несколько различных морфологических теггеров. Как же узнать, какой использовать? Давайте сравним их качество!\n",
    "\n",
    "В этой домашке вам будет нужно найти тексты на русском языке (размер корпуса не менее 200 слов), в которых будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их вручную – с помощью этих текстов мы будем оценивать качество работы наших теггеров. В текстах размечаем только части речи, ничего больше!\n",
    "\n",
    "(1 балл) Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки (какие моменты вы тут считаете трудными для автоматического посттеггинга и почему, в этом вам может помочь второй ридинг). Не забывайте, что разные теггеры могут использовать разные тегсеты: напишите комментарий о том, какой тегсет вы берёте для разметки и почему.\n",
    "\n",
    "(3 балла) Потом вам будет нужно взять три POS теггера для русского языка (udpipe, stanza, natasha, pymorphy, mystem, spacy, deeppavlov) и «прогнать» текст через каждый из них.\n",
    "\n",
    "(2 балла) Затем оценим accuracy для каждого теггера. Заметьте, что в разных системах имена тегов и части речи могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции-конвертера и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции.\n",
    "\n",
    "(4 балла) Дальше вам нужно взять лучший теггер для русского языка и с его помощью написать функцию (chunker), которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд) В предыдущем дз многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. Балл за объяснение того, почему именно эти группы вы взяли, балл за создание такого рода чанкера, балл за за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без.\n",
    "\n",
    "### FAQ\n",
    "В каком формате должна быть ручная разметка корпуса? - В любом, как вам удобно (табличка токен + тег, conllu, и тд). Не забудьте загрузить его на гитхаб вместе с решением.\n",
    "\n",
    "Верно ли, что корпусом может быть набор не связанных между собой предложений? - Да.\n",
    "\n",
    "Если случай спорный, у него может быть два разбора или нужно принять единое решение? - Может быть два разбора, если так подсказывает ваша лингвистическая интуиция.\n",
    "\n",
    "Как оценивать accuracy, если POS-теггер не разрешает омонимию? - Проверить, что это точно так. За правильные ответы тогда можно считать случаи, когда тег из gold-разметки есть в ответе теггера, либо как это часто бывает, брать первый (нулевой) вариант - you decide.\n",
    "\n",
    "Что делать со штуками вроде причастия? Если теггер указывает его как глагол, а мы разметили как причастие, это стоит засчитать (потому что у теггера не было возможности узнать наш тегсет) или нет? Или как раз на моменте выбора тегсета нужно продумать его так, чтобы по возможности избежать таких ситуаций? - Можно при конвертации тегов посмотреть, не нужно ли учитывать для этого теггера не только POS-тег, но и какие-то другие граммемы, чтобы потом преобразовать этот тег в причастие. Собственно, для таких случаев и нужна какая-то более хитрая конвертация, а не просто словарик соответствий POS-тегов.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4327c9-8e11-4496-890f-d15a4a55c152",
   "metadata": {},
   "source": [
    "# Концепт"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09cfab-fc41-4bc1-9169-51b309f2473a",
   "metadata": {},
   "source": [
    "Прочитав про корпус нестандартных слов, я в ту же секунду подумала о всяких словотворческих экспериментах вроде хрюкотающих (*хрюкочущих?.. кажется, я тут не сильно лучше морфопарсера*) зелюков в мове и глокой куздры. \n",
    "\n",
    "Но я решила пожалеть себя и парсеры и взять что-то, обладающее некоторым количеством реальных слов. Посему мой корпус состоит исключительно из стихотворений Велимира Хлебникова (не сильно лучше и не очень оригинально, согласна).\n",
    "\n",
    "Почему этот корпус более-менее ок для теста разметки?\n",
    "- выдуманные и редкие слова (*в вырей стаи времирей умчались*, *осокоревые кущи*)\n",
    "- слова с дефисами (в том числе и выдуманные слова с дефисами) \n",
    "- некоторое количество омонимов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52d91f-ffde-47ea-a2d4-6b64084ecfe8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Импорты и первичная токенизация "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788f0eb8-01f8-41b6-b093-7a2ae0e40dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "from string import punctuation\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581c6e0a-b327-4418-b4db-9f30e4ca24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7ff6cd-2a23-48e9-89e1-aa0bc451ec57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "mstem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d979871e-46d0-4601-a95c-a98286a4c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь просто был код для того, чтобы побить текст на токены\n",
    "\n",
    "# poems_file= open('poems.txt', 'r', encoding='UTF-8')\n",
    "# poems_text = poems_file.read()\n",
    "# tokens = [word.lower() for word in word_tokenize(poems_text) if word not in punctuation]\n",
    "# with open('markup.csv', 'w', newline='', encoding=\"UTF-8\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for token in tokens:\n",
    "#         writer.writerow([token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b4d47-3d8e-4041-9519-9fbd8c6117f8",
   "metadata": {},
   "source": [
    " # Тэгсет\n",
    " Вот мой общий тэгсет, в который я планирую радостно переводить все полученные всеми тэггерами тэги.\n",
    " Частично он был вдохновлён предыдущим соревновательным ридингом, частично чем-то, что мне иногда кажется здравым смыслом.\n",
    " \n",
    " NOUN - существительные,\n",
    " \n",
    " ADJ - прилагательные,\n",
    " \n",
    " VERB - глаголы, а ещё причастия и деепричастия,\n",
    " \n",
    " ADV - наречия, а ещё частицы и междометия,\n",
    " \n",
    " PRON - местоимения,\n",
    " \n",
    " PREP - предлоги, \n",
    " \n",
    " CONJ - союзы,\n",
    " \n",
    " NUM - числительные\n",
    " \n",
    " Я просто посмотрела на тэгсеты pymorphy, spacy и mystem и решила собрать из них один единый словарь значений, соответствующих моим выделенным тэгам, это показалось логичнее, чем прописывать три отдельных словаря/три отдельные функции.\n",
    " \n",
    "<!--     \n",
    "pymorphy: \n",
    "PARTS_OF_SPEECH = frozenset([\n",
    "        'NOUN',  # имя существительное\n",
    "        'ADJF',  # имя прилагательное (полное)\n",
    "        'ADJS',  # имя прилагательное (краткое)\n",
    "        'COMP',  # компаратив\n",
    "        'VERB',  # глагол (личная форма)\n",
    "        'INFN',  # глагол (инфинитив)\n",
    "        'PRTF',  # причастие (полное)\n",
    "        'PRTS',  # причастие (краткое)\n",
    "        'GRND',  # деепричастие\n",
    "        'NUMR',  # числительное\n",
    "        'ADVB',  # наречие\n",
    "        'NPRO',  # местоимение-существительное\n",
    "        'PRED',  # предикатив\n",
    "        'PREP',  # предлог\n",
    "        'CONJ',  # союз\n",
    "        'PRCL',  # частица\n",
    "        'INTJ',  # междометие\n",
    "    ])\n",
    "    \n",
    "    \n",
    "spacy: \n",
    "POS_LIST = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\", \"SPACE\"]\n",
    "    \n",
    "mystem:  \n",
    "A прилагательное\n",
    "ADV наречие\n",
    "ADVPRO местоименное наречие\n",
    "ANUM числительное-прилагательное\n",
    "APRO местоимение-прилагательное\n",
    "COM часть композита - сложного слова\n",
    "CONJ союз\n",
    "INTJ междометие\n",
    "NUM числительное\n",
    "PART частица\n",
    "PR предлог\n",
    "S существительное\n",
    "SPRO местоимение-существительное\n",
    "V глагол -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f7e115b-eb32-43ef-aec9-7def2a98f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {'NOUN':['NOUN', 'PROPN', 'S'], \n",
    "                'ADJ': ['ADJF', 'ADJS', 'COMP', 'ADJ', 'A'],\n",
    "                'VERB': ['VERB', 'INFN', 'PRTF', 'PRTS', 'GRND', 'AUX', 'V'],\n",
    "                'ADV': ['ADVB', 'PRCL', 'INTJ', 'ADV', 'PART'], \n",
    "                'PRON': ['NPRO', 'DET', 'PRON', 'ADVPRO', 'APRO', 'SPRO'], \n",
    "                'PREP': ['PREP', 'ADP', 'PR'],\n",
    "                'CONJ': ['CONJ', 'CCONJ', 'SCONJ'],\n",
    "                'NUM': ['NUMR', 'NUM', 'ANUM']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7413a187-a157-482d-addc-bf6b2af17d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'PRON', 'CONJ']\n"
     ]
    }
   ],
   "source": [
    "def convert_tags(tag_list):\n",
    "    converted_tags = []\n",
    "    for tag in tag_list:\n",
    "        for ult_tag, tag_options in convert_dict.items():\n",
    "            if tag in tag_options:\n",
    "                converted_tags.append(ult_tag)\n",
    "                break\n",
    "        else:\n",
    "            converted_tags.append(tag)\n",
    "    return converted_tags\n",
    "\n",
    "\n",
    "print(convert_tag(['A', 'SPRO', 'SCONJ'])) # просто тестовый принт для проверки работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "af6a8c59-31ff-49d2-a11a-c70ae7ca6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []\n",
    "annot_pos = []\n",
    "with open('markup.csv', 'r', encoding = \"UTF-8\") as f: \n",
    "    file = csv.reader(f)\n",
    "    for row in file: \n",
    "        tokens_list.append(row[0])\n",
    "        annot_pos.append(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce6ab9-3edb-4ff1-b5fd-4ff0a11b1267",
   "metadata": {},
   "source": [
    "# Pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624e5ab-a699-44c2-a697-07dd146e3199",
   "metadata": {},
   "source": [
    "Дальше для каждого из выбранных тэггеров всё будет выглядеть примерно одинаково. \n",
    "\n",
    "Я паршу токены, вылавливаю оттуда POS-тэг, записываю в лист тэгов и конвертирую этот лист во что-то более универсальное с помощью ранее сделанного словарика тэгов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5242aa52-ecb0-477e-b177-8107a2f1c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmrph_tags = []\n",
    "for token in tokens_list:\n",
    "    parsed = morph.parse(token)\n",
    "    pmrph_tags.append(str(parsed[0].tag.POS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1957abbb-1eb3-4a29-a0a3-2608b9a1262d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pymorphy_tags = convert_tags(pmrph_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab219c0f-0ba0-4b37-af1d-3505aa2f2a7f",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fca0f2a7-dc19-4df1-a20e-baf70362ab12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.7.0/ru_core_news_md-3.7.0-py3-none-any.whl (41.9 MB)\n",
      "     --------------------------------------- 41.9/41.9 MB 31.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\python38\\lib\\site-packages (from ru-core-news-md==3.7.0) (3.7.1)\n",
      "Requirement already satisfied: pymorphy3>=1.0.0 in c:\\python38\\lib\\site-packages (from ru-core-news-md==3.7.0) (1.2.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\python38\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.7.0) (0.7.2)\n",
      "Requirement already satisfied: docopt-ng>=0.6 in c:\\python38\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in c:\\python38\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.7.0) (2.4.417150.4580142)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2.26.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (3.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\python38\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python38\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python38\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.7.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python38\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\python38\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python38\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in c:\\python38\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python38\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-md==3.7.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('ru_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1efed3fd-f440-474a-b0e3-ab018c43d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tags = []\n",
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "doc = nlp(' '.join(tokens_list))\n",
    "for token in doc:\n",
    "    spacy_tags.append(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "82005972-1350-464e-9b0c-8a410892f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tags = convert_tags(spacy_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b616e9c-a356-4ce4-90be-b63a8c996754",
   "metadata": {},
   "source": [
    "# Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "93bf981d-8956-446a-8807-1a5cc1e95441",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mstem.analyze(' '.join(tokens_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a087d109-2e4c-4080-b089-c9c4efd3d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_tags = []\n",
    "words_annotated = []\n",
    "for word in a:\n",
    "    if 'analysis' in word:\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        mystem_tags.append(pos)\n",
    "        words_annotated.append([word['text'], pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1527fcbb-b4ba-4937-b602-b01a6278cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_tags = convert_tags(mystem_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44dac3-00f6-40a5-8682-8149158f6f08",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d43e032f-85cb-417b-b7a7-bb7c55839e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy: 0.9122137404580153\n",
      "Spacy: 0.8664122137404581\n",
      "Mystem: 0.9541984732824428\n"
     ]
    }
   ],
   "source": [
    "print(\"Pymorphy:\", accuracy_score(pymorphy_tags, annot_pos))\n",
    "print(\"Spacy:\", accuracy_score(spacy_tags, annot_pos))\n",
    "print(\"Mystem:\", accuracy_score(mystem_tags, annot_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b6f7b-c741-4db8-8118-06703b46c311",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c7a5b-7c73-40d9-a7e8-31c9f6291bf5",
   "metadata": {},
   "source": [
    "Лучшим тэггером, в итоге, оказался Mystem. Это немножко грустно, поскольку у него очень неудобный формат выдачи, потому придётся пострадать (и попробовать отредактировать один из предыдущих кусков кода так, что у нас будет список, маппящий слова к тэгам). \n",
    "\n",
    "(Тут я уже исходила из того, что конвертация тэгов нам не очень нужна, потому довольствуемся майстемовским тэгсетом)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a07d0b64-6c64-4615-835f-356470c5207d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['в', 'PR'], ['пору', 'S'], ['когда', 'CONJ'], ['в', 'PR'], ['вырей', 'S']]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_annotated[:5] # сниппет того, как выглядит пословная разметка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75046bc5-e235-4d68-859e-9e88e00c5100",
   "metadata": {},
   "source": [
    "Напишу простой и относительно красивый чанкер только для частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a2e0ee08-3c14-4205-8dff-f70af40643c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(tag1, tag2):\n",
    "    bigram_list = []\n",
    "    for word in words_annotated:\n",
    "        if tag1 == word[1]:\n",
    "            next_word = words_annotated[words_annotated.index(word)+1] # поиск по индексу следующего элемента\n",
    "            if next_word[1] == tag2:\n",
    "                bigram = word[0] + ' ' + next_word[0]\n",
    "                bigram_list.append(bigram)\n",
    "    return bigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2d41501d-7657-4bf0-a083-3de68e24334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['томный круг', 'нежная дитяти', 'небесных прав', 'туманных веко', 'малиновому рту', 'осокоревые кущи', 'суровым садом', 'черноокой горожанки', 'медлительной южанки', 'белого казани', 'низок севастополь', 'целый год', 'гордый тополь', 'прямодушнее туркмена', 'милая измена', 'звонким смехом']\n"
     ]
    }
   ],
   "source": [
    "print(chunker('A', 'S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c5115-1217-455d-874b-58dc7a90e924",
   "metadata": {},
   "source": [
    "Теперь напишу спагетти-ифовый чанкер, который, кажется, больше отвечает тз, чем предыдущий.\n",
    "\n",
    "В него будут подаваться две строки (либо POS'ы, либо слово + POS) и третья строка, которая выбирает мод (pos, wordpos, posword). \n",
    "Теоретически, можно было бы сделать автоматическую подачу мода путём простого просмотра, есть ли поданное слово в списке POS (у нас выше есть converted_dict, можно было бы итерироваться по его items), но я не успеваю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3c387536-e8aa-41f7-b27c-dda4a71b7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# available modes - pos, wordpos, posword\n",
    "def chunker_dif(el1, el2, mode):\n",
    "    bigram_list = []\n",
    "    for word in words_annotated:\n",
    "         # поиск по индексу следующего элемента\n",
    "        if mode == \"pos\":\n",
    "            if el1 == word[1]:\n",
    "                next_word = words_annotated[words_annotated.index(word)+1] \n",
    "                if next_word[1] == el2:\n",
    "                    bigram = word[0] + ' ' + next_word[0]\n",
    "                    bigram_list.append(bigram)\n",
    "        elif mode == \"wordpos\":\n",
    "            if el1 == word[0]:\n",
    "                next_word = words_annotated[words_annotated.index(word)+1] \n",
    "                if next_word[1] == el2:\n",
    "                    bigram = word[0] + ' ' + next_word[0]\n",
    "                    bigram_list.append(bigram)\n",
    "        elif mode == \"posword\":\n",
    "            if el1 == word[1]:\n",
    "                next_word = words_annotated[words_annotated.index(word)+1] \n",
    "                if next_word[0] == el2: \n",
    "                    bigram = word[0] + ' ' + next_word[0]\n",
    "                    bigram_list.append(bigram)\n",
    "    return bigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "99a3a30d-9fa9-4370-b992-d59306091010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['вырей времирей', 'времушком камушком', 'времушек камушек', 'времушко камушко', 'времыня крылья', 'открыватель истин', 'круг прамин', 'прамин бога', 'кольцо потяти', 'бог длинноты', 'птицы вечности', 'кольцо потяти', 'боги чашу', 'волну истоков', 'истоков эксампей', 'главу дерзавицу', 'тело кружева', 'кружева изнанка', 'колыбели мотылька', 'жизни радуги', 'цвету север', 'север запад', 'южанки руку', 'березой ветвь', 'ветвь дева', 'указаний кремля', 'казани стены', 'стены битвою']\n"
     ]
    }
   ],
   "source": [
    "print(chunker_dif('S', 'S', 'pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d9811e2e-acbb-4aab-baa8-4dccb8fa94fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['целый год']\n"
     ]
    }
   ],
   "source": [
    "print(chunker_dif('целый', 'S', 'wordpos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5cd550e5-a92c-4141-be91-5bb1c9abe1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['томный круг']\n"
     ]
    }
   ],
   "source": [
    "print(chunker_dif('A', 'круг', 'posword'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d52c66-9baf-4e3b-84e0-92dc89ea8605",
   "metadata": {},
   "source": [
    "Ну, выглядит это так, будто бы оно работает. \n",
    "\n",
    "В случае с отзывами, я бы предложила посмотреть на такие шаблоны:\n",
    "- \"не\" + ADV/ADJ (не очень, не лучший/худший)\n",
    "- \"не\" + VERB (не дотягивает, не работает) \n",
    "- ADJ + \"но\" (хороший, но/плохой, но) \n",
    "- ADV + ADJ (e.g. невероятно хороший)\n",
    "\n",
    "Кажется, что отрицания в принципе должны встречаться чаще, но можно смотреть на предположительную тональность второго слова и в большинстве случаев инвертировать её (лучший -> не лучший -> отзыв скорее негативен). \n",
    "\n",
    "В случае с ADV + ADJ, кажется, в большинстве случаев оценка тональности коллокации будет совпадать с оцекой тональности ADV  (навскидку, такая комбинация будет встречаться в очень эмоциональных отзывах, но какой окраской они будут обладать, не вполне ясно + вещи вроде \"ужасно интересный\" - их тоже можно прописать)\n",
    "\n",
    "\n",
    "(И нет, я не заимплементила это для предыдущей домашки, хотя очень хотелось :()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef967a-bcf7-4f43-8c0e-b81f76e6cb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
